{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I use Python 3 and Natural Language Processing to build a spam detection filter. I use a dataset from UCI's Machine Learning repository which contains a collection of over 5000 sms phone messages of spam and ham (normal text message). \n",
    "\n",
    "In the beginning I modify, analyze, and visualize the data. In the second part, I begin to preprocess the data. The data is in the text format of strings, and our classification algorithm need numerical feature vector to perform classification task. Bag of words is a method in which each unique word in the text is represented as one number. I use CountVectorizer, Tdidftransformer, and Multinomial NB, all to perform the pre processing (bag of words method, term fand classification,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download_shell() stopwords library needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "messages = [line.rstrip() for line in open('smsspamcollection/SMSSpamCollection')]\n",
    "print(len(messages)) #grabs a list of only messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham\\tWhat you thinked about me. First time you saw me in class.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "viewing messages first lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "\n",
      "\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "\n",
      "\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n",
      "\n",
      "\n",
      "5 spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv\n",
      "\n",
      "\n",
      "6 ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "\n",
      "\n",
      "7 ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "\n",
      "\n",
      "8 spam\tWINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "\n",
      "\n",
      "9 spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message_no, message in enumerate(messages[:10]):\n",
    "    print(message_no, message)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing the first ten messages I can see some are actually real text messages, while others are spam and claim the user as a winner or try to get your money.\n",
    "\n",
    "The spacing in text 8 indicates that it is a TSV (tab separated values file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages = pd.read_csv('smsspamcollection/SMSSpamCollection',sep='\\t',names=['label','message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use pandas library to separate the text and create a nice dataframe separated by a labels column and message column wherever a tab occurs. Otherwise I would have to manually parse the entire TSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.describe() # can see that there are repeated common text message by unique row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I can analyze how many spam vs ham messages and the most common text message strings in these messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages['length'] = messages['message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d970188518>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naaW2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d97007a780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages['length'].plot.hist(bins=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.602477\n",
       "std        60.119851\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       920.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest fallingObstacles for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[messages['length'] == 920]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exploring outliers with pandas. The outlier appears to be a love letter sent over text which is why it has such a high character count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x000001D970467B00>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x000001D970591780>], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuUAAAEQCAYAAAAXjQrJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSY0ZyiVJkqTG/j+xuwshmaS1TQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d9703ea080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.hist(column='length',by='label',bins=60,figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ham vs Spam comparison. It appears that spam messages tend to have more characters. There seems to be an average of about 50 while spam is about 150. Visualiing the data I see that length could possibly be a good feature to use for our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to convert text format (strings) into numbers so that our classification algrithms can use a numerical feature vector to perform the task. I first need to remove common words the, a, if, (stopwords) by utilizing NLTK library.\n",
    "Next, I create a function that splits a message into individual words and returns a list. I then use apply() in pandas to process the text in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mess = 'Sample message! Notice: it has punctuation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nopunc = [c for c in mess if c not in string.punctuation] #check for punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'a',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'm',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'a',\n",
       " 'g',\n",
       " 'e',\n",
       " ' ',\n",
       " 'N',\n",
       " 'o',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 't',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'u',\n",
       " 'n',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\Christopher Truong/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Christopher Truong\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Christopher Truong\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Christopher Truong\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords.zip/stopwords/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\Christopher Truong/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Christopher Truong\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Christopher Truong\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Christopher Truong\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-ff9cd17f22b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\Christopher Truong/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Christopher Truong\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Christopher Truong\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Christopher Truong\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nopunc = ''.join(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nopunc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_mess = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterated through the the list for word in nopunc.split if lowercase version is not in stop words it removed it and has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['message'].head(5).apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many methods for normalizing text. The reason why vectorization works well here is because our datasets has a lot of short hand and abbreviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have each message represented as a list of tokens, I need to convert them into a vector for SciKit Learn. The bag of words model process has these steps:\n",
    "\n",
    "1. Counting word appearance in the message (term frequency)\n",
    "2. Weighing the counts (frequent tokens have lower weight this is called inverse document frequency).\n",
    "3. Normalizing the vectors to unit length, to abstract from the original text length (L2 norm)\n",
    "\n",
    "Each vector will have as many dimensions as there are unique qwords in the SMS corpus.\n",
    "\n",
    "1. SciKit Learn count vecterizer estimator object model will convert a collection of text documents to a matrix of token counts (bag of words model)\n",
    "2. Two dimentsional matrix one dimensionis entire vocabulary one row per word \n",
    "3. The other dimension are the actual documnets or a column per text message. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mess4 = messages['message'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mess4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow4 = bow_transformer.transform([mess4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 unique words in message for after removing the common stop words. 2 of them appear twice everything else appears only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer.get_feature_names()[4068]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of the Sparse Matrix',messages_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages_bow.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is a weight commonly used in information retrieval and text mining. It is a statistical measure used to evaluate the importance of a word to a document or collection. Importance increases proportionally to its appearances in the document; however, is offset by the frequency in the corpus. This is used to often in search engines and are a central tool in scores and rank of a documents relevance for user queries. \n",
    "\n",
    "Typically has two terms: first term computes number of appearances in the document divided by total words in the document.\n",
    "The second term is the Inverse Document Frequency which is computed as the logarithm of the number of documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf4 = tfidf_transformer.transform(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-5aca937bd889>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf4' is not defined"
     ]
    }
   ],
   "source": [
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformed simple word count into term frequency inverse document frequency. Interpreted as weight values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-23abcb1387a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midf_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbow_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'u'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midf_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbow_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'university'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_transformer' is not defined"
     ]
    }
   ],
   "source": [
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['u']])\n",
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['university']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 11425)\n"
     ]
    }
   ],
   "source": [
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've set up the messages as vectors now I move on with training the spam and ham classifier. The algorithm used is the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_detect_model = MultinomialNB().fit(messages_tfidf,messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_detect_model.predict(tfidf4)[0]    #testing it on a single message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['label'][3]   #confirms that the model worked in this instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'spam' ..., 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "all_pred = spam_detect_model.predict(messages_tfidf)\n",
    "print(all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      1.00      0.99      4825\n",
      "       spam       1.00      0.85      0.92       747\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(messages['label'],all_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model was evaluated on the same data we used on training. It was to show an example but in general we never use the same dataset that was trained on. The limitations of this is that we don't know the predictive power of new messages. In order to truly evaluate a train test split needs to be performed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msg_train, msg_test, label_train, label_test = train_test_split(messages['message'], messages['label'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4242                 Detroit. The home of snow. Enjoy it.\n",
       "2370    A Boy loved a gal. He propsd bt she didnt mind...\n",
       "5235    Am on the uworld site. Am i buying the qbank o...\n",
       "2581    U are subscribed to the best Mobile Content Se...\n",
       "4905    no, i *didn't* mean to post it. I wrote it, an...\n",
       "2836    Ya they are well and fine., BBD(pooja) full pi...\n",
       "107           Aight, I'll hit you up when I get some cash\n",
       "4592    Well done ENGLAND! Get the official poly ringt...\n",
       "4841    PRIVATE! Your 2003 Account Statement for shows...\n",
       "2193    Congrats ! Treat pending.i am not on mail for ...\n",
       "4436    Don't b floppy... b snappy & happy! Only gay c...\n",
       "313     He says he'll give me a call when his friend's...\n",
       "1086    FR'NDSHIP is like a needle of a clock. Though ...\n",
       "2007    Shopping lor. Them raining mah hard 2 leave or...\n",
       "5127                      Cuz ibored. And don wanna study\n",
       "4609                  We live in the next  &lt;#&gt; mins\n",
       "5261    I absolutely LOVE South Park! I only recently ...\n",
       "4163    How's it going? Got any exciting karaoke type ...\n",
       "3665     Huh? 6 also cannot? Then only how many mistakes?\n",
       "3261    I'm always looking for an excuse to be in the ...\n",
       "1941    Dude avatar 3d was imp. At one point i thought...\n",
       "1942    WELL DONE! Your 4* Costa Del Sol Holiday or £5...\n",
       "1586    I was wondering if it would be okay for you to...\n",
       "2728                  Whatsup there. Dont u want to sleep\n",
       "0       Go until jurong point, crazy.. Available only ...\n",
       "1458    CLAIRE here am havin borin time & am now alone...\n",
       "5394            I dont know exactly could you ask chechi.\n",
       "263     MY NO. IN LUTON 0125698789 RING ME IF UR AROUN...\n",
       "919     Hey you gave them your photo when you register...\n",
       "38                            Anything lor... U decide...\n",
       "                              ...                        \n",
       "2820    Don't forget who owns you and who's private pr...\n",
       "3966                         Love you aathi..love u lot..\n",
       "2803    And smile for me right now as you go and the w...\n",
       "3       U dun say so early hor... U c already then say...\n",
       "796      it's really getting me down just hanging around.\n",
       "3803            No da. I am happy that we sit together na\n",
       "3027      I finished my lunch already. U wake up already?\n",
       "5403             So gd got free ice cream... I oso wan...\n",
       "4025                                Wat time ü wan today?\n",
       "557              Having lunch:)you are not in online?why?\n",
       "5530    I think that tantrum's finished so yeah I'll b...\n",
       "1844                Super da:)good replacement for murali\n",
       "2801    House-Maid is the murderer, coz the man was mu...\n",
       "4739    I bought the test yesterday. Its something tha...\n",
       "1594    PRIVATE! Your 2003 Account Statement for shows...\n",
       "3132    LookAtMe!: Thanks for your purchase of a video...\n",
       "909                       WHITE FUDGE OREOS ARE IN STORES\n",
       "1296                            TELL HER I SAID EAT SHIT.\n",
       "403                  The hair cream has not been shipped.\n",
       "2466                       S.i think he is waste for rr..\n",
       "2860    Do you know why god created gap between your f...\n",
       "3426                      True. Its easier with her here.\n",
       "588     Pete can you please ring meive hardly gotany c...\n",
       "5251    Yeah work is fine, started last week, all the ...\n",
       "520     Usually the person is unconscious that's in ch...\n",
       "3524                                      Try neva mate!!\n",
       "2506                 Congrats kano..whr s the treat maga?\n",
       "4502                                So wat's da decision?\n",
       "3547    SO IS TH GOWER MATE WHICH IS WHERE I AM!?! HOW...\n",
       "779     Happy New year my dear brother. I really do mi...\n",
       "Name: message, Length: 3900, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=text_process)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',MultinomialNB())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bow', CountVectorizer(analyzer=<function text_process at 0x1a11e7cf28>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None...f=False, use_idf=True)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.96      1.00      0.98      1450\n",
      "       spam       1.00      0.70      0.83       222\n",
      "\n",
      "avg / total       0.96      0.96      0.96      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(label_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline method streamlines the previous transformation steps for all the data. We can directly pass message text data and pipeline will do the pre-processing. The classification report for our true testing set indicates a high accuracy rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
